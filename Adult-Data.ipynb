{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f22b9-b235-4f87-a3b1-dcec14a354db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Callable\n",
    "import os\n",
    "import gc\n",
    "import traceback\n",
    "import warnings\n",
    "import zipfile\n",
    "from pdb import set_trace\n",
    "\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3937c7-cf42-4049-9bb8-ac65bed3b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"adult.zip\"\n",
    "\n",
    "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"income\"]\n",
    "\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    # Load training data\n",
    "    df_train = pd.read_csv(\n",
    "        z.open(\"adult.data\"),\n",
    "        names=columns,\n",
    "        sep=\",\",\n",
    "        na_values=\"?\",\n",
    "        skipinitialspace=True\n",
    "    )\n",
    "    \n",
    "    # Load test data (skip first line because it contains a comment)\n",
    "    df_test = pd.read_csv(\n",
    "        z.open(\"adult.test\"),\n",
    "        names=columns,\n",
    "        sep=\",\",\n",
    "        na_values=\"?\",\n",
    "        skipinitialspace=True,\n",
    "        skiprows=1\n",
    "    )\n",
    "\n",
    "#df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "#df[\"income\"] = df[\"income\"].str.replace(\".\", \"\", regex=False)\n",
    "#df[\"income\"] = df[\"income\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b273399e-0160-4167-a608-517b22e52d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "sns.pairplot(df_train[[\"age\", \"education_num\", \"hours_per_week\", \"income\"]], hue=\"income\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6a1a7-f9ea-4348-84b4-8b661fb6d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=df_train, x=\"workclass\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf7575-b468-47c7-be05-0b88003f38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.countplot(data=df_train, x=\"education\", hue=\"income\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be13f7-5ade-42bb-8988-eae355fd594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    data=df_train,\n",
    "    x=\"age\",\n",
    "    hue=\"sex\",\n",
    "    bins=30,\n",
    "    multiple=\"stack\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a92193-c84e-43a0-b093-ad1bfaf3b10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_train_valid_test_data(X: np.ndarray, y: np.ndarray, ):\n",
    "    \"\"\" Randomizes and then splits the data into train, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            X: Data given as a 2D matrix\n",
    "\n",
    "            y: Labels given as a vector \n",
    "    \"\"\"\n",
    "    X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, train_size=.8, random_state=42)\n",
    "    X_trn, X_vld, y_trn, y_vld = train_test_split(X_trn, y_trn, train_size=.8, random_state=42)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst\n",
    "\n",
    "def get_preprocessed_data() -> Tuple[np.ndarray]:\n",
    "    \"\"\" Gets preprocessed data for training, validation, and testing\n",
    "\n",
    "        Returns:\n",
    "            A tuple of NumPy arrays where indices 0-1 \n",
    "            contain the training data/targets, indices 2-3\n",
    "            contain the validation data/targets, and 4-5\n",
    "            contain the testing data/targets.\n",
    "    \"\"\"\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = None, None, None, None, None, None\n",
    "    # Drop the label column and store the features (pixel values) in X\n",
    "    X = df.drop('label', axis=1).values\n",
    "    y = df['label'].values\n",
    "    \n",
    "    # TODO 3.1 - 3.4\n",
    "    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "    y = df['label'].values.reshape(-1,1)\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "    X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = get_train_valid_test_data(X, y_encoded)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_trn = scaler.fit_transform(X_trn)\n",
    "    X_vld = scaler.transform(X_vld)\n",
    "    X_tst = scaler.transform(X_tst)\n",
    "\n",
    "    def add_bias(X):\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        return np.hstack((bias, X))\n",
    "\n",
    "    X_trn = add_bias(X_trn)\n",
    "    X_vld = add_bias(X_vld)\n",
    "    X_tst = add_bias(X_tst)\n",
    "\n",
    "    return X_trn, y_trn, X_vld, y_vld, X_tst, y_tst\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    softreg: object, \n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray,\n",
    "    xlabel: str = '',\n",
    "    ylabel: str = '',\n",
    "    class_names: Dict = None\n",
    ") -> None:\n",
    "    \"\"\" Plots the decision boundry for data with 2 features. \n",
    "    \n",
    "        Warning: \n",
    "            If you have more than 2 features (2D data) the decision boundry\n",
    "            can not be plotted.\n",
    "    \n",
    "        Args:\n",
    "            softreg: An instance of SoftmaxRegression class\n",
    "            \n",
    "            X: Data to be plotted\n",
    "\n",
    "            y: Labels for corresponding data\n",
    "\n",
    "            xlabel: X-axis label for plot\n",
    "\n",
    "            ylabel: Y-axis label for plot\n",
    "            \n",
    "            class_names: Dictionary mapping labels to class names.\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 2, f\"`X` must have 2 features not {X.shape[-1]}\"\n",
    "        \n",
    "    if class_names is None:\n",
    "        class_names = {}\n",
    "\n",
    "    # Generate fake data to cover entire space of our input features X\n",
    "    buffer = .5\n",
    "    x_min, x_max = X[:, 0].min() - buffer, X[:, 0].max() + buffer\n",
    "    y_min, y_max = X[:, 1].min() - buffer, X[:, 1].max() + buffer\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n",
    "                         np.arange(y_min, y_max, .02))\n",
    "    fake_data = np.c_[xx.ravel(), yy.ravel()].reshape(-1,2)\n",
    "    fake_data =  np.hstack([np.ones((len(fake_data), 1)), fake_data])\n",
    "    # Make prediction\n",
    "    y_hat = softreg.predict(fake_data)\n",
    "\n",
    "    # Plot\n",
    "    plt.contourf(xx, yy, y_hat.reshape(xx.shape))\n",
    "    \n",
    "    labels = np.unique(y)\n",
    "    for l in labels:\n",
    "        class_locs = np.where(y == l)[0]\n",
    "        class_name = class_names.get(l, f'class {l}')\n",
    "        plt.scatter(X[class_locs, 0], X[class_locs, 1], label=class_name)\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y: np.ndarray, \n",
    "    y_hat: np.ndarray, \n",
    "    class_names: Dict[int, str] = None,\n",
    "    figsize: Tuple = (10, 5)\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" Plots a pretty and labeld version of Sklerarn's confusion matrix\n",
    "\n",
    "        Args:\n",
    "            y: Ground truth labels given as a 1D vector\n",
    "\n",
    "            y_hat: Predicted labels given as a 1D vector\n",
    "\n",
    "            class_names: Dictionary mapping labels to class names.\n",
    "\n",
    "                Example: {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "\n",
    "            figsize: A tuple for setting the width and height of the plot.\n",
    "\n",
    "        Returns:\n",
    "            A confusion matrix casted as a DataFrame\n",
    "    \"\"\"\n",
    "    y =  y.flatten() # reshape to make 1D vector for consistency\n",
    "    y_hat = y_hat.flatten() # reshape to make 1D vector for consistency\n",
    "    \n",
    "    cfm = confusion_matrix(y_true=y, y_pred=y_hat)    \n",
    "    \n",
    "    labels = np.sort(np.unique(y))\n",
    "    if class_names is not None:\n",
    "        classes = []\n",
    "        for l in labels:\n",
    "            class_name = class_names.get(l, l)\n",
    "            classes.append(class_name)\n",
    "        labels = classes\n",
    "        \n",
    "    columns, index = labels, labels\n",
    "    fig, ax = plt.subplots(figsize=figsize)  \n",
    "    cfm_df = pd.DataFrame(cfm, index=index, columns=columns)\n",
    "    sns.heatmap(cfm_df, annot=True, fmt='g', ax=ax)\n",
    "    plt.show()\n",
    "    return cfm_df\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    # TODO 4\n",
    "    return np.exp(z) / (1 + np.exp(z))\n",
    "    pass\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Computes the stablized version of the softmax\n",
    "\n",
    "        Args:\n",
    "            z: A vector or matrix of continuous values.\n",
    "\n",
    "        Returns:\n",
    "            A NumPy array with the same shape as the input.\n",
    "    \"\"\"\n",
    "    # TODO 5\n",
    "    z_max = np.max(z, axis=1, keepdims=True)\n",
    "\n",
    "    exp_z = np.exp(z - z_max)\n",
    "\n",
    "    softmax_values = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    return softmax_values\n",
    "\n",
    "    pass\n",
    "\n",
    "def nll_loss(y: np.ndarray, probs: np.ndarray) -> float:\n",
    "    \"\"\" Computes the average generalized NLL Loss\n",
    "\n",
    "        Args:\n",
    "            y: The ground truth one-hot encoded labels\n",
    "                given as matrix of shape (N, K).\n",
    "\n",
    "            probs: The predicted probabilities for the\n",
    "                corresponding labels given as a matrix\n",
    "                of shape (N, K)\n",
    "        Returns:\n",
    "            A NLL loss given as a float.\n",
    "    \"\"\"\n",
    "    # TODO 6\n",
    "\n",
    "    eps = 1e-15\n",
    "    probs = np.clip(probs, eps, 1 - eps)\n",
    "\n",
    "    nll = -np.mean(np.sum(y * np.log(probs), axis=1))\n",
    "\n",
    "    return nll\n",
    "    pass\n",
    "\n",
    "def get_batches(\n",
    "    data_len: int, \n",
    "    batch_size: int = 32,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\" Generates mini-batches based on the data indexes\n",
    "        \n",
    "        Args:\n",
    "            data_len: Length of the data or number of data samples \n",
    "                in the data. This is used to generate the indices of\n",
    "                the data.\n",
    "            \n",
    "            batch_size: Size of each mini-batch where the last mini-batch\n",
    "                might be smaller than the rest if the batch_size does not \n",
    "                evenly divide the data length.\n",
    "\n",
    "        Returns:\n",
    "            A list of NumPy array's holding the indices of batches\n",
    "    \"\"\"\n",
    "    indices = np.arange(data_len)\n",
    "    np.random.shuffle(indices)\n",
    "    batches = [indices[i:i+batch_size] for i in range(0, data_len, batch_size)]\n",
    "\n",
    "    return batches\n",
    "\n",
    "class SoftmaxRegression():\n",
    "    \"\"\" Performs softmax regression using gradient descent\n",
    "    \n",
    "        Attributes:\n",
    "\n",
    "            alpha: learning rate or step size.\n",
    "                \n",
    "            batch_size: Size of mini-batches for mini-batch gradient\n",
    "                descent.\n",
    "            \n",
    "            epochs: Number of epochs to run for mini-batch\n",
    "                gradient descent.\n",
    "                \n",
    "            seed: Seed to be used for NumPy's RandomState class\n",
    "                or universal seed np.random.seed() function.\n",
    "\n",
    "            W: Matrix of weights with shape (M, K) \n",
    "\n",
    "            trn_loss: Stores the training loss for each epoch.\n",
    "\n",
    "            vld_loss: Stores the validation loss for each epoch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        alpha: float,\n",
    "        batch_size: int,\n",
    "        epochs: int = 1,\n",
    "        seed: int = 0,\n",
    "    ):\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.W = None\n",
    "        self.trn_loss = None\n",
    "        self.vld_loss = None\n",
    "    \n",
    "    def fit(\n",
    "         self, \n",
    "         X: np.ndarray, \n",
    "         y: np.ndarray, \n",
    "         X_vld: np.ndarray=None, \n",
    "         y_vld: np.ndarray=None\n",
    "     ) -> object:\n",
    "        \"\"\" Trains softmax regressio using SGD\n",
    "        \n",
    "            Args:\n",
    "                X: Training data given as a 2D matrix\n",
    "\n",
    "                y: Training labels given as a 2D column vector\n",
    "\n",
    "                X_vld: Validation data given as a 2D matrix. Used \n",
    "                    to compute the validation accuracy for each epoch.\n",
    "\n",
    "                y_vld: Validation labels given as a 2D matrix. Used \n",
    "                    to compute the validation accuracy for each epoch.\n",
    "                \n",
    "            Returns:\n",
    "                The class's own object reference. \n",
    "        \"\"\"\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        self.trn_loss = []\n",
    "        self.vld_loss = []\n",
    "\n",
    "        # TODO 7.1\n",
    "        n_features = X.shape[1]\n",
    "        n_classes = y.shape[1]\n",
    "        self.W = np.random.rand(n_features, n_classes)\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            batches = get_batches(len(X), self.batch_size)\n",
    "\n",
    "            for b in batches:\n",
    "                X_b = X[b]\n",
    "                y_b = y[b]\n",
    "                Z = X_b @ self.W\n",
    "                Y_hat = softmax(Z)\n",
    "                grad_W = (X_b.T @ (Y_hat - y_b)) / X_b.shape[0]\n",
    "                self.W -= self.alpha * grad_W\n",
    "\n",
    "            trn_probs = softmax(X @ self.W)\n",
    "            trn_loss = nll_loss(y, trn_probs)\n",
    "            self.trn_loss.append(trn_loss)\n",
    "            \n",
    "            if X_vld is not None and y_vld is not None:\n",
    "                vld_probs = softmax(X_vld @ self.W)\n",
    "                vld_loss = nll_loss(y_vld, vld_probs)\n",
    "                self.vld_loss.append(vld_loss)\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Make predictions using learned weights\n",
    "\n",
    "            Args:\n",
    "                X: Testing data given as a 2D matrix\n",
    "\n",
    "            Returns:\n",
    "                A 2D column vector of predictions for each data sample in X\n",
    "        \"\"\"\n",
    "        # TODO 7.2\n",
    "        probs = softmax(X @ self.W)\n",
    "        preds = np.argmax(probs, axis=1).reshape(-1, 1)\n",
    "        return preds\n",
    "\n",
    "        pass\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "# TODO 8: Training Code\n",
    "X_trn, y_trn, X_vld, y_vld, X_tst, y_tst = get_preprocessed_data()\n",
    "\n",
    "model = SoftmaxRegression(alpha=0.05, batch_size=64, epochs=20, seed=42)\n",
    "model.fit(X_trn, y_trn, X_vld, y_vld)\n",
    "\n",
    "y_trn_pred = model.predict(X_trn)\n",
    "\n",
    "y_trn_true = np.argmax(y_trn, axis=1).reshape(-1, 1)\n",
    "\n",
    "train_acc = accuracy_score(y_trn_true, y_trn_pred)\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(model.trn_loss, label=\"Training Loss\", linewidth=2)\n",
    "plt.plot(model.vld_loss, label=\"Validation Loss\", linewidth=2)\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"NLL Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "cm_trn = confusion_matrix(y_trn_true, y_trn_pred)\n",
    "disp_trn = ConfusionMatrixDisplay(confusion_matrix=cm_trn, display_labels=list(label2name.values()))\n",
    "disp_trn.plot(cmap='Blues', xticks_rotation=45)\n",
    "plt.title(\"Training Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "y_vld_pred = model.predict(X_vld)\n",
    "y_vld_true = np.argmax(y_vld, axis=1).reshape(-1, 1)\n",
    "\n",
    "val_acc = accuracy_score(y_vld_true, y_vld_pred)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "cm_vld = confusion_matrix(y_vld_true, y_vld_pred)\n",
    "disp_vld = ConfusionMatrixDisplay(confusion_matrix=cm_vld, display_labels=list(label2name.values()))\n",
    "disp_vld.plot(cmap='Oranges', xticks_rotation=45)\n",
    "plt.title(\"Validation Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# TODO 8: Testing Code\n",
    "y_tst_pred = model.predict(X_tst)\n",
    "y_tst_true = np.argmax(y_tst, axis=1).reshape(-1, 1)\n",
    "\n",
    "test_acc = accuracy_score(y_tst_true, y_tst_pred)\n",
    "print(f\"Testing Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "cm_tst = confusion_matrix(y_tst_true, y_tst_pred)\n",
    "disp_tst = ConfusionMatrixDisplay(confusion_matrix=cm_tst, display_labels=list(label2name.values()))\n",
    "disp_tst.plot(cmap='Greens', xticks_rotation=45)\n",
    "plt.title(\"Testing Confusion Matrix\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
